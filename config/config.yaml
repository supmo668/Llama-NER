data:
  dataset_name: "eriktks/conll2003"
  max_length: 128
  batch_size: 8

model:
  base_model: "unsloth/Llama-3.2-1B-Instruct"
  # "bert-base-uncased" 
  num_labels: 9 
  freeze_backbone: true  # or false

training:
  lr: 5e-5
  epochs: 3
  gpus: 1
  log_dir: "lightning_logs"
  checkpoint_dir: "checkpoints"
  use_checkpoint: false
  losses:
    loss_function: "compound_loss"  
    # Options: 'cross_entropy', 'focal_loss', 'label_smoothing', 'compound_loss'
    compound_loss:
      cross_entropy_weight: 0.5
      crf_weight: 0.3
      label_smoothing_weight: 0.2
    focal_loss:
      alpha: 1.0
      gamma: 2.0
    label_smoothing:
      smoothing: 0.1

evaluation:
  load_checkpoint: true  # Whether to load the latest checkpoint for evaluation

advanced:
  use_crf: true
  crf_dropout: 0.1
  peft: lora
